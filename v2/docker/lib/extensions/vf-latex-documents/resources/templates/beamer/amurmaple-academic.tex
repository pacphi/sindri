\documentclass{beamer}
\usetheme[sidebar,leftframetitle]{Amurmaple}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[backend=biber,style=ieee,citestyle=numeric-comp]{biblatex}

% Bibliography
\addbibresource{references.bib}

% Metadata
\title{Research Presentation Title}
\subtitle{A Comprehensive Study}
\author{Author Name\inst{1} \and Co-Author Name\inst{2}}
\institute{
  \inst{1} Department of Computer Science, University A \\
  \inst{2} Research Institute, University B
}
\date{Conference Name, \today}

\mail{author@university.edu}
\webpage{https://research.university.edu}
\collaboration{Joint work with Research Group X}

\begin{document}

% Title page
\frame{\titlepage}

% Abstract
\begin{frame}{Abstract}
  \begin{abstract}
    This presentation introduces a novel approach to solving problem X.
    We demonstrate theoretical foundations, implement practical solutions,
    and validate results through comprehensive experiments. Our findings
    show significant improvements over existing methods.
  \end{abstract}
\end{frame}

% Table of Contents
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% Introduction
\section{Introduction}

\sepframe

\begin{frame}{Background}
  \framesection{Problem Statement}

  \begin{itemize}
    \item Context and motivation
    \item Current challenges in the field
    \item Gaps in existing research
  \end{itemize}

  \framesection{Research Questions}

  \begin{enumerate}
    \item How can we improve upon existing methods?
    \item What are the theoretical limits?
    \item Does this scale to real-world applications?
  \end{enumerate}
\end{frame}

\begin{frame}{Related Work}
  \begin{columns}[T]
    \begin{column}{0.48\textwidth}
      \textbf{Classical Approaches}
      \begin{itemize}
        \item Method A \cite{reference1}
        \item Method B \cite{reference2}
        \item Limitations of traditional methods
      \end{itemize}
    \end{column}
    \begin{column}{0.48\textwidth}
      \textbf{Recent Advances}
      \begin{itemize}
        \item Deep learning approaches
        \item Hybrid methods
        \item Open challenges
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

% Methodology
\section{Methodology}

\sepframe

\begin{frame}{Theoretical Framework}
  \framesection{Mathematical Foundation}

  Consider the optimization problem:
  \begin{equation}
    \min_{x \in \mathbb{R}^n} f(x) = \sum_{i=1}^{m} \ell(y_i, h(x_i; \theta))
  \end{equation}

  where:
  \begin{itemize}
    \item $\ell(\cdot)$ is the loss function
    \item $h(\cdot; \theta)$ is our model with parameters $\theta$
    \item $(x_i, y_i)$ are training samples
  \end{itemize}
\end{frame}

\begin{frame}{Algorithm}
  \begin{block}{Proposed Algorithm}
    \begin{algorithmic}[1]
      \STATE Initialize parameters $\theta_0$
      \FOR{$t = 1$ to $T$}
        \STATE Sample mini-batch $\mathcal{B}_t$
        \STATE Compute gradient: $g_t = \nabla_\theta \mathcal{L}(\theta_t; \mathcal{B}_t)$
        \STATE Update: $\theta_{t+1} = \theta_t - \alpha g_t$
      \ENDFOR
      \RETURN $\theta_T$
    \end{algorithmic}
  \end{block}
\end{frame}

% Results
\section{Results}

\sepframe

\begin{frame}{Experimental Setup}
  \framesection{Datasets}

  \begin{table}
    \caption{Dataset characteristics}
    \centering
    \begin{tabular}{lccc}
      \toprule
      Dataset & Samples & Features & Classes \\
      \midrule
      Dataset A & 10,000 & 128 & 10 \\
      Dataset B & 50,000 & 256 & 100 \\
      Dataset C & 100,000 & 512 & 1000 \\
      \bottomrule
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}{Performance Comparison}
  \begin{columns}[T]
    \begin{column}{0.48\textwidth}
      \begin{figure}
        \centering
        % \includegraphics[width=\textwidth]{results_plot.pdf}
        \caption{Accuracy vs. training time}
      \end{figure}
    \end{column}
    \begin{column}{0.48\textwidth}
      \begin{block}{Key Results}
        \begin{itemize}
          \item 15\% accuracy improvement
          \item 3x faster convergence
          \item Better generalization
        \end{itemize}
      \end{block}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Statistical Analysis}
  \begin{theorem}[Convergence Rate]
    Under assumptions A1-A3, the proposed algorithm achieves convergence
    rate $O(1/\sqrt{T})$ where $T$ is the number of iterations.
  \end{theorem}

  \begin{proof}[Proof sketch]
    By applying Lyapunov analysis and using the strong convexity
    assumption, we can show that the expected error decreases at
    rate $O(1/\sqrt{T})$.
  \end{proof}
\end{frame}

% Discussion
\section{Discussion}

\sepframe

\begin{frame}{Interpretation}
  \framesection{Main Findings}

  \begin{itemize}
    \item Our method significantly outperforms baselines
    \item Theoretical guarantees match empirical observations
    \item Scalability to large datasets confirmed
  \end{itemize}

  \framesection{Limitations}

  \begin{itemize}
    \item Computational cost for very high dimensions
    \item Requires careful hyperparameter tuning
    \item Limited to supervised learning scenarios
  \end{itemize}
\end{frame}

% Conclusion
\section{Conclusion}

\sepframe

\begin{frame}{Summary}
  \begin{enumerate}
    \item Introduced novel framework for problem X
    \item Provided theoretical convergence guarantees
    \item Demonstrated superior empirical performance
    \item Identified directions for future research
  \end{enumerate}

  \bigskip

  \begin{block}{Future Work}
    \begin{itemize}
      \item Extension to unsupervised settings
      \item Application to domain Y
      \item Development of adaptive variants
    \end{itemize}
  \end{block}
\end{frame}

% References
\begin{frame}[allowframebreaks]{References}
  \printbibliography[heading=none]
\end{frame}

% Thank you
\thanksframe{Thank you for your attention!\\ Questions?}

% Appendix (optional)
\appendix

\section{Additional Material}

\begin{frame}{Additional Results}
  \begin{itemize}
    \item Supplementary experiments
    \item Detailed proofs
    \item Implementation details
  \end{itemize}
\end{frame}

\end{document}
